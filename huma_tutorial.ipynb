{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Huma AI tutorial\n",
    "\n",
    "Congratulation you have successfully installed all software and started the Jupiter notebook with the full Huma tutorial. Now you can start to learn and tryout out yourself how the AI of Huma works. If you are not yet amilier with jupyter I would advice you to watch this [tutorial](https://youtu.be/jZ952vChhuI) on youtube. But a jupyter notebook is very initiative. Execute a cell of code you just have to hit the play botton when the cell is selected. If you go through the tutorial you just execute one cell after the other. Be aware that some later cells are dependent on the execution of previous cells.\n",
    "\n",
    "## 1.0 What is the AI behind Huma?\n",
    "\n",
    "Neural networks and especially deep once with lots of layers had in the last years made a big impact on the predictive capabilities of the models they can produce. The neural networks that are most well known are convolutional neural networks that allow us to detect e.g. objects in images. But there are similar advances in building neutral networks around patterns in sequences of data. That AI capability  would be the perfect match, if we want to predict customer intent based on a sequence of their behavior they exposes. For predictive models around sequence data recurrent networks and in particular the special recurrent network long short-term memory (LSTM) show impressive results in many business applications, why it is time for us to understand how it can be used for Huma.\n",
    "\n",
    "\n",
    "### 1.1 Sequence Prediction Problems\n",
    "\n",
    "Sequence prediction, we want to use in Huma, is different to other types of supervised learning problems. The sequence imposes an order on the observations that must be preserved when training models and making predictions. Generally, prediction problems that involves sequence data are referred to as sequence prediction problems, although there are a suite of problems that differ based on the input and output sequences. For Huma we want to consider the following two types of sequence prediction approaches:\n",
    "\n",
    "1. Sequence-to-sequence prediction and\n",
    "\n",
    "2. Sequence classiﬁcation.\n",
    "\n",
    "Other types of sequence prediction approaches would be sequence generation and a sequence prediction where the prediction is just the next value in the sequence. This class of sequence prediction is therefore just a special case for the sequence-to-sequence prediction.\n",
    "\n",
    "#### 1.1.1 Sequence-to-Sequence Prediction\n",
    "\n",
    "Sequence-to-sequence prediction involves predicting an output sequence given an input sequence. For example based on a customer click event sequence we predict the most likely future click event sequence:\n",
    "\n",
    "Input Sequence: LANDING_PAGE,LANDING_PAGE,LANDING_PAGE,DETAIL_PAGE\n",
    "Output Sequence: \n",
    "SHOPPING_CART,PURCHASE\n",
    "\n",
    "![Sequence-to-Sequence Model](graphics/Sequence-to-Sequence_Model.png)\n",
    "\n",
    "The sequence-to-sequence prediction, predicts based on a sequence a new sequence that may or may not have the same length as the input sequence.\n",
    "\n",
    "#### 1.1.2 Sequence Classiﬁcation\n",
    "\n",
    "Sequence classification involves predicting a class label for a given input sequence. For example we can think in the Huma context of a classification of a click event sequence to a particular persona or typical customer situation:\n",
    "\n",
    "Input Sequence: DETAIL_PAGE, SHOPPING_CART, PURCHASE\n",
    "Output Sequence: \"Customer knows what he wants\"\n",
    "\n",
    "![Sequence Classification Model](graphics/Sequence_Classification_Model.png)\n",
    "\n",
    "So our objective of sequence classification here is to build a classification model using a labeled dataset, so that the model can be used to predict the class label of an unseen sequence.\n",
    "\n",
    "### 1.2 What Recurrent Networks (RNN) can do?\n",
    "\n",
    "Recurrent Neural Networks (RNNs), are a special type of neural network designed for sequence problems. Given a standard feedforward MLP network, an RNN can be thought of as the addition of loops to the architecture. For example, in a given layer, each neuron may pass its signal latterly (sideways) in addition to forward to the next layer. The output of the network may feedback as an input to the network with the next input vector. And so on.\n",
    "\n",
    "The recurrent connections add state or memory to the network and allow it to learn and harness the ordered nature of observations within input sequences.\n",
    "\n",
    "The addition of sequence is a new dimension to the function being approximated. Instead of mapping inputs to outputs alone, the network is capable of learning a mapping function for the inputs over time to an output. The internal memory can mean outputs are conditional on the recent context in the input sequence, not what has just been presented as input to the network. In a sense, this capability unlocks time series for neural networks.\n",
    "\n",
    "In addition to the general benefits of using neural networks for sequence prediction, RNNs can also learn and harness the temporal dependence from the data. In the simplest case, the network is shown one observation at a time from a sequence and can learn what observations are relevant that it has seen previously and how they are relevant for making a prediction.\n",
    "\n",
    "The promise of recurrent neural networks is that the temporal dependence and contextual information in the input data can be learned, and this is a crucial enabler to make use of RNNs for the prediction of customer behavior and intent, what has a temporal dependence on behavior of the customer too.\n",
    "\n",
    "### 1.3 What makes LTSM a special RNN?\n",
    "\n",
    "\n",
    "The LSTM network is different to a classical Deep Feed Forward (DFF) network. \n",
    "\n",
    "![Deep Feed Forward](graphics/Deep_Feed_Forward.png)\n",
    "\n",
    "Like an Deep Feed Forward network, the LSTM network is comprised of layers of neurons. Input data is propagated through the network in order to make a prediction.\n",
    "\n",
    "Like RNNs, the LSTMs have recurrent connections so that the state from previous activations of the neuron from the previous time step is used as context for formulating an output. \n",
    "\n",
    "![Recurrent_Neural_Network](graphics/Recurrent_Neural_Network.png)\n",
    "\n",
    "But unlike other RNNs, the LSTM has a unique formulation that allows it to avoid the problems that prevent the training and scaling of other RNNs. This, and the impressive results that can be achieved, are the reason for the popularity of the technique.\n",
    "\n",
    "The key technical historical challenge faced with RNNs is how to train them effectively. Experiments show how difficult this was where the weight update procedure resulted in weight changes that quickly became so small as to have no effect (vanishing gradients) or so large that they result in very large changes or even overﬂow (exploding gradients). LSTMs overcome this challenge by design.\n",
    "\n",
    "![LSTM](graphics/LSTM.png)\n",
    "\n",
    "The computational unit of the LSTM network is called the memory cell, memory block, or just cell for short. The term neuron as the computational unit is so ingrained when describing DFFs that it is often used to refer to the LSTM memory cell. LSTM cells are comprised of weights and gates.\n",
    "\n",
    "![LSTM Cell](graphics/LSTM_Cell.png)\n",
    "\n",
    "*LSTM Weights*\n",
    "\n",
    "A memory cell has weight parameters for the input, output, as well as an internal state that is built up through exposure to input time steps.\n",
    "\n",
    "- Input Weights: Used to weight input for the current time step.\n",
    "- Output Weights: Used to weight the output from the last time step.\n",
    "- Internal State: Internal state used in the calculation of the output for this time step.\n",
    "\n",
    "*LSTM Gates*\n",
    "\n",
    "The key to the memory cell are the gates. These too are weighted functions that further govern the information ﬂow in the cell. There are three gates:\n",
    "\n",
    "- Forget Gate: Decides what information to discard from the cell.\n",
    "\n",
    "- Input Gate: Decides which values from the input to update the memory state.\n",
    "\n",
    "- Output Gate: Decides what to output based on input and the memory of the cell.\n",
    "\n",
    "The forget gate and input gate are used in the updating of the internal state. The output gate is a final limiter on what the cell actually outputs. It is these gates and the consistent data ﬂow called the constant error carrousel or CEC that keep each cell stable (neither exploding or vanishing).\n",
    "\n",
    "Unlike a traditional DFF neuron, it is hard to draw an LSTM memory unit cleanly. There are lines, weights, and gates all over the place. The 3 key benefits of LSTMs are:\n",
    "\n",
    "- Overcomes the technical problems of training an RNN, namely vanishing and exploding gradients.\n",
    "- Possesses memory to overcome the issues of long-term temporal dependency with input sequences.\n",
    "- Process input sequences and output sequences time step by time step, allowing variable length inputs and outputs.\n",
    "\n",
    "## 2.0 How to predict customers next click(s)\n",
    "\n",
    "If we want to predict the click behavior of a customer, we need to forecasting the next click after a one or better multiple clicks that happend before. That problem can be framed as a sequence of one input time step to one output time step (one-to-one) or multiple input time steps to one output time step (many-to-one) sequence prediction problem.\n",
    "\n",
    "![LSTM Cell](graphics/Event_Timeline.png)\n",
    "\n",
    "But it is not so valuable, if we can just predict the next click, it would rather be more valuable if we could predict a sequence of customer clicks that will happen after a sequence in the past. \n",
    "\n",
    "![LSTM Cell](graphics/Predict_More_Steps.png)\n",
    "\n",
    "\n",
    "The prediction of a sequence base on another sequence, is more challenging. One modeling concern that makes these problems challenging, is that the length of the input and output sequences can vary. Given that there are multiple input time steps and multiple output time steps, we refer here to as many-to-many type sequence prediction problem.\n",
    "\n",
    "### 2.1 Data preparation for simulated customer click behavior data\n",
    "\n",
    "Before we can create a model that is able to make a sequence-to-sequence prediction, we first need to create some data that we can train and test the model on. As I did not find online real world data to use for my experiment, I considered to create the training data myself, based on the assumptions, that customers are showing different click behavior.\n",
    "\n",
    "To create the click data, I created a statistical model how different type of customers would behave on a e-commerce site. My artificial e-commerce site has following pages:\n",
    "\n",
    "1. SEARCH_PAGE\n",
    "2. LANDING_PAGE\n",
    "3. DETAIL_PAGE\n",
    "4. CATEGORY_PAGE\n",
    "5. BLOG_POST\n",
    "6. WISH_LIST\n",
    "7. SHOPPING_CART\n",
    "8. PURCHASE\n",
    "\n",
    "And for each of the following envisioned customer personas:\n",
    "\n",
    "1. looking_for_bargain\n",
    "2. looking_for_quality\n",
    "3. looking_for_inspiration\n",
    "4. does_not_know_what_to_get\n",
    "5. shops_for_family\n",
    "6. looking_for_wedding_shopping\n",
    "\n",
    "I created a page transition matrix, which describes in each row the probability, that this type of customer will navigate from the page named in the row to one of the pages listed in the column of the matrix. All the page transition matrix are found in this folder ./page_transition_matrix and are named \"page_transition_matrix-<name of the customer persona>.csv\". And following I list all the table for your review to see the different patterns I assumed for the different customer personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports of panda to load csv and displaying the csv tables in jupyter\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "# import of numpy to use the functions in the lib to statistically \n",
    "# calcuatle what the next page will be base on the define probabilites\n",
    "import numpy as np\n",
    "# import of random function for picking randomly personas for the test data creation\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <th>BLOG_POST</th>\n",
       "      <th>WISH_LIST</th>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <th>PURCHASE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAGES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOG_POST</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WISH_LIST</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEARCH_PAGE  LANDING_PAGE  DETAIL_PAGE  CATEGORY_PAGE  \\\n",
       "PAGES                                                                  \n",
       "SEARCH_PAGE            0.3           0.1          0.5            0.1   \n",
       "LANDING_PAGE           0.0           0.3          0.5            0.2   \n",
       "DETAIL_PAGE            0.0           0.2          0.5            0.1   \n",
       "CATEGORY_PAGE          0.0           0.2          0.5            0.3   \n",
       "BLOG_POST              0.0           0.0          0.0            0.0   \n",
       "WISH_LIST              0.0           0.0          0.0            0.0   \n",
       "SHOPPING_CART          0.2           0.3          0.0            0.0   \n",
       "PURCHASE               0.0           0.0          0.0            0.0   \n",
       "\n",
       "               BLOG_POST  WISH_LIST  SHOPPING_CART  PURCHASE  \n",
       "PAGES                                                         \n",
       "SEARCH_PAGE            0          0            0.0       0.0  \n",
       "LANDING_PAGE           0          0            0.0       0.0  \n",
       "DETAIL_PAGE            0          0            0.2       0.0  \n",
       "CATEGORY_PAGE          0          0            0.0       0.0  \n",
       "BLOG_POST              0          0            0.0       0.0  \n",
       "WISH_LIST              0          0            0.0       0.0  \n",
       "SHOPPING_CART          0          0            0.0       0.5  \n",
       "PURCHASE               0          0            0.0       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# page transation matrix for persona \"looking_for_bargain\"\n",
    "df_page_transition_matrix_looking_for_bargain = pd.read_csv(\"./page_transition_matrix/page_transition_matrix-looking_for_bargain.csv\" , sep=';', decimal= \",\")\n",
    "df_page_transition_matrix_looking_for_bargain = df_page_transition_matrix_looking_for_bargain.set_index(\"PAGES\")\n",
    "display(df_page_transition_matrix_looking_for_bargain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <th>BLOG_POST</th>\n",
       "      <th>WISH_LIST</th>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <th>PURCHASE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAGES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOG_POST</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WISH_LIST</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEARCH_PAGE  LANDING_PAGE  DETAIL_PAGE  CATEGORY_PAGE  \\\n",
       "PAGES                                                                  \n",
       "SEARCH_PAGE            0.3             0          0.5            0.2   \n",
       "LANDING_PAGE           0.0             0          0.0            0.0   \n",
       "DETAIL_PAGE            0.0             0          0.2            0.5   \n",
       "CATEGORY_PAGE          0.0             0          0.5            0.5   \n",
       "BLOG_POST              0.0             0          0.0            0.0   \n",
       "WISH_LIST              0.0             0          0.0            0.0   \n",
       "SHOPPING_CART          0.0             0          0.0            0.5   \n",
       "PURCHASE               0.0             0          0.0            0.0   \n",
       "\n",
       "               BLOG_POST  WISH_LIST  SHOPPING_CART  PURCHASE  \n",
       "PAGES                                                         \n",
       "SEARCH_PAGE            0          0            0.0       0.0  \n",
       "LANDING_PAGE           0          0            0.0       0.0  \n",
       "DETAIL_PAGE            0          0            0.3       0.0  \n",
       "CATEGORY_PAGE          0          0            0.0       0.0  \n",
       "BLOG_POST              0          0            0.0       0.0  \n",
       "WISH_LIST              0          0            0.0       0.0  \n",
       "SHOPPING_CART          0          0            0.0       0.5  \n",
       "PURCHASE               0          0            0.0       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# page transation matrix for persona \"looking_for_quality\"\n",
    "df_page_transition_matrix_looking_for_quality = pd.read_csv(\"./page_transition_matrix/page_transition_matrix-looking_for_quality.csv\" , sep=';', decimal= \",\")\n",
    "df_page_transition_matrix_looking_for_quality = df_page_transition_matrix_looking_for_quality.set_index(\"PAGES\")\n",
    "display(df_page_transition_matrix_looking_for_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <th>BLOG_POST</th>\n",
       "      <th>WISH_LIST</th>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <th>PURCHASE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAGES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOG_POST</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WISH_LIST</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASE</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEARCH_PAGE  LANDING_PAGE  DETAIL_PAGE  CATEGORY_PAGE  \\\n",
       "PAGES                                                                  \n",
       "SEARCH_PAGE           0.30          0.05          0.5           0.10   \n",
       "LANDING_PAGE          0.10          0.00          0.5           0.15   \n",
       "DETAIL_PAGE           0.15          0.00          0.2           0.15   \n",
       "CATEGORY_PAGE         0.20          0.00          0.5           0.20   \n",
       "BLOG_POST             0.25          0.00          0.3           0.10   \n",
       "WISH_LIST             0.10          0.00          0.4           0.00   \n",
       "SHOPPING_CART         0.10          0.00          0.0           0.00   \n",
       "PURCHASE              0.10          0.00          0.4           0.00   \n",
       "\n",
       "               BLOG_POST  WISH_LIST  SHOPPING_CART  PURCHASE  \n",
       "PAGES                                                         \n",
       "SEARCH_PAGE         0.05       0.00           0.00      0.00  \n",
       "LANDING_PAGE        0.10       0.05           0.10      0.00  \n",
       "DETAIL_PAGE         0.00       0.30           0.20      0.00  \n",
       "CATEGORY_PAGE       0.00       0.00           0.05      0.05  \n",
       "BLOG_POST           0.20       0.10           0.05      0.00  \n",
       "WISH_LIST           0.00       0.00           0.50      0.00  \n",
       "SHOPPING_CART       0.00       0.10           0.00      0.80  \n",
       "PURCHASE            0.00       0.00           0.50      0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# page transation matrix for persona \"looking_for_inspiration\"\n",
    "df_page_transition_matrix_looking_for_inspiration = pd.read_csv(\"./page_transition_matrix/page_transition_matrix-looking_for_inspiration.csv\" , sep=';', decimal= \",\")\n",
    "df_page_transition_matrix_looking_for_inspiration = df_page_transition_matrix_looking_for_inspiration.set_index(\"PAGES\")\n",
    "display(df_page_transition_matrix_looking_for_inspiration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <th>BLOG_POST</th>\n",
       "      <th>WISH_LIST</th>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <th>PURCHASE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAGES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOG_POST</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WISH_LIST</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASE</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEARCH_PAGE  LANDING_PAGE  DETAIL_PAGE  CATEGORY_PAGE  \\\n",
       "PAGES                                                                  \n",
       "SEARCH_PAGE           0.30          0.05         0.20           0.40   \n",
       "LANDING_PAGE          0.20          0.00         0.15           0.50   \n",
       "DETAIL_PAGE           0.40          0.00         0.15           0.40   \n",
       "CATEGORY_PAGE         0.20          0.00         0.20           0.60   \n",
       "BLOG_POST             0.30          0.00         0.00           0.25   \n",
       "WISH_LIST             0.00          0.00         0.00           0.00   \n",
       "SHOPPING_CART         0.95          0.00         0.00           0.00   \n",
       "PURCHASE              0.00          0.00         0.00           0.00   \n",
       "\n",
       "               BLOG_POST  WISH_LIST  SHOPPING_CART  PURCHASE  \n",
       "PAGES                                                         \n",
       "SEARCH_PAGE         0.05          0           0.00      0.00  \n",
       "LANDING_PAGE        0.15          0           0.00      0.00  \n",
       "DETAIL_PAGE         0.00          0           0.05      0.00  \n",
       "CATEGORY_PAGE       0.00          0           0.00      0.00  \n",
       "BLOG_POST           0.45          0           0.00      0.00  \n",
       "WISH_LIST           0.00          0           0.00      0.00  \n",
       "SHOPPING_CART       0.00          0           0.00      0.05  \n",
       "PURCHASE            0.00          0           0.00      0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# page transation matrix for persona \"does_not_know_what_to_get\"\n",
    "df_page_transition_matrix_does_not_know_what_to_get = pd.read_csv(\"./page_transition_matrix/page_transition_matrix-does_not_know_what_to_get.csv\" , sep=';', decimal= \",\")\n",
    "df_page_transition_matrix_does_not_know_what_to_get = df_page_transition_matrix_does_not_know_what_to_get.set_index(\"PAGES\")\n",
    "display(df_page_transition_matrix_does_not_know_what_to_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <th>BLOG_POST</th>\n",
       "      <th>WISH_LIST</th>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <th>PURCHASE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAGES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOG_POST</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WISH_LIST</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEARCH_PAGE  LANDING_PAGE  DETAIL_PAGE  CATEGORY_PAGE  \\\n",
       "PAGES                                                                  \n",
       "SEARCH_PAGE            0.3           0.0          0.5            0.2   \n",
       "LANDING_PAGE           0.0           0.0          0.6            0.4   \n",
       "DETAIL_PAGE            0.3           0.0          0.1            0.3   \n",
       "CATEGORY_PAGE          0.2           0.0          0.5            0.3   \n",
       "BLOG_POST              0.0           0.0          0.5            0.5   \n",
       "WISH_LIST              0.0           0.0          0.0            0.0   \n",
       "SHOPPING_CART          0.4           0.0          0.0            0.3   \n",
       "PURCHASE               0.0           0.0          0.0            0.0   \n",
       "\n",
       "               BLOG_POST  WISH_LIST  SHOPPING_CART  PURCHASE  \n",
       "PAGES                                                         \n",
       "SEARCH_PAGE          0.0        0.0            0.0       0.0  \n",
       "LANDING_PAGE         0.0        0.0            0.0       0.0  \n",
       "DETAIL_PAGE          0.0        0.0            0.3       0.0  \n",
       "CATEGORY_PAGE        0.0        0.0            0.0       0.0  \n",
       "BLOG_POST            0.0        0.0            0.0       0.0  \n",
       "WISH_LIST            0.0        0.0            0.0       0.0  \n",
       "SHOPPING_CART        0.0        0.0            0.0       0.3  \n",
       "PURCHASE             0.0        0.0            0.0       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# page transation matrix for persona \"shops_for_family\"\n",
    "df_page_transition_matrix_shops_for_family = pd.read_csv(\"./page_transition_matrix/page_transition_matrix-shops_for_family.csv\" , sep=';', decimal= \",\")\n",
    "df_page_transition_matrix_shops_for_family = df_page_transition_matrix_shops_for_family.set_index(\"PAGES\")\n",
    "display(df_page_transition_matrix_shops_for_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <th>BLOG_POST</th>\n",
       "      <th>WISH_LIST</th>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <th>PURCHASE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAGES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOG_POST</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WISH_LIST</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURCHASE</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SEARCH_PAGE  LANDING_PAGE  DETAIL_PAGE  CATEGORY_PAGE  \\\n",
       "PAGES                                                                  \n",
       "SEARCH_PAGE           0.15             0         0.70           0.15   \n",
       "LANDING_PAGE          0.00             0         0.00           0.80   \n",
       "DETAIL_PAGE           0.00             0         0.39           0.30   \n",
       "CATEGORY_PAGE         0.00             0         0.50           0.50   \n",
       "BLOG_POST             0.00             0         0.30           0.50   \n",
       "WISH_LIST             0.10             0         0.40           0.00   \n",
       "SHOPPING_CART         0.00             0         0.00           0.20   \n",
       "PURCHASE              0.00             0         0.00           0.00   \n",
       "\n",
       "               BLOG_POST  WISH_LIST  SHOPPING_CART  PURCHASE  \n",
       "PAGES                                                         \n",
       "SEARCH_PAGE            0        0.0           0.00       0.0  \n",
       "LANDING_PAGE           0        0.2           0.00       0.0  \n",
       "DETAIL_PAGE            0        0.3           0.01       0.0  \n",
       "CATEGORY_PAGE          0        0.0           0.00       0.0  \n",
       "BLOG_POST              0        0.2           0.00       0.0  \n",
       "WISH_LIST              0        0.0           0.50       0.0  \n",
       "SHOPPING_CART          0        0.3           0.00       0.5  \n",
       "PURCHASE               0        0.0           0.00       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# page transation matrix for persona \"looking_for_wedding_shopping\"\n",
    "df_page_transition_matrix_looking_for_wedding_shopping = pd.read_csv(\"./page_transition_matrix/page_transition_matrix-looking_for_wedding_shopping.csv\" , sep=';', decimal= \",\")\n",
    "df_page_transition_matrix_looking_for_wedding_shopping = df_page_transition_matrix_looking_for_wedding_shopping.set_index(\"PAGES\")\n",
    "display(df_page_transition_matrix_looking_for_wedding_shopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities that a persona will open a particular page first, I defined in the separate table in csv \"persona_page_bias_matrix.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEARCH_PAGE</th>\n",
       "      <th>LANDING_PAGE</th>\n",
       "      <th>DETAIL_PAGE</th>\n",
       "      <th>CATEGORY_PAGE</th>\n",
       "      <th>BLOG_POST</th>\n",
       "      <th>WISH_LIST</th>\n",
       "      <th>SHOPPING_CART</th>\n",
       "      <th>PURCHASE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERSONAS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>looking_for_bargain</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking_for_quality</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking_for_inspiration</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does_not_know_what_to_get</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops_for_family</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking_for_wedding_shopping</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              SEARCH_PAGE  LANDING_PAGE  DETAIL_PAGE  \\\n",
       "PERSONAS                                                               \n",
       "looking_for_bargain                  0.00           0.9         0.10   \n",
       "looking_for_quality                  0.00           0.0         0.20   \n",
       "looking_for_inspiration              0.05           0.1         0.15   \n",
       "does_not_know_what_to_get            0.00           0.9         0.00   \n",
       "shops_for_family                     0.30           0.1         0.25   \n",
       "looking_for_wedding_shopping         0.00           0.2         0.00   \n",
       "\n",
       "                              CATEGORY_PAGE  BLOG_POST  WISH_LIST  \\\n",
       "PERSONAS                                                            \n",
       "looking_for_bargain                    0.00        0.0        0.0   \n",
       "looking_for_quality                    0.80        0.0        0.0   \n",
       "looking_for_inspiration                0.30        0.4        0.0   \n",
       "does_not_know_what_to_get              0.10        0.0        0.0   \n",
       "shops_for_family                       0.25        0.1        0.0   \n",
       "looking_for_wedding_shopping           0.30        0.0        0.5   \n",
       "\n",
       "                              SHOPPING_CART  PURCHASE  \n",
       "PERSONAS                                               \n",
       "looking_for_bargain                       0         0  \n",
       "looking_for_quality                       0         0  \n",
       "looking_for_inspiration                   0         0  \n",
       "does_not_know_what_to_get                 0         0  \n",
       "shops_for_family                          0         0  \n",
       "looking_for_wedding_shopping              0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The matix with the probabilities that a person will open a particular page first\n",
    "df_persona_page_bias_matrix = pd.read_csv(\"persona_page_bias_matrix.csv\", sep=';', decimal= \",\")\n",
    "df_persona_page_bias_matrix = df_persona_page_bias_matrix.set_index(\"PERSONAS\")\n",
    "display(df_persona_page_bias_matrix)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical model defined for each persona, defines its most likely click behavior. Therefore e.g. we will see in the navigation path of a persona that does wedding shopping more likely a click on the wishlist as for all the other personas.\n",
    "\n",
    "All personas that I defined I captured also in a separate file, to loop through it, when we need to create click path for each persona randomly. Following you see the content of the personas.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persona_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>shopper_type</th>\n",
       "      <th>age_range</th>\n",
       "      <th>income</th>\n",
       "      <th>family_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>m</td>\n",
       "      <td>looking_for_bargain</td>\n",
       "      <td>millennials</td>\n",
       "      <td>low</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sara</td>\n",
       "      <td>f</td>\n",
       "      <td>looking_for_quality</td>\n",
       "      <td>baby-boomer</td>\n",
       "      <td>high</td>\n",
       "      <td>married</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linda</td>\n",
       "      <td>f</td>\n",
       "      <td>looking_for_inspiration</td>\n",
       "      <td>generation-x</td>\n",
       "      <td>medium</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tom</td>\n",
       "      <td>m</td>\n",
       "      <td>does_not_know_what_to_get</td>\n",
       "      <td>millennials</td>\n",
       "      <td>single</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lisa</td>\n",
       "      <td>f</td>\n",
       "      <td>shops_for_family</td>\n",
       "      <td>generation-x</td>\n",
       "      <td>high</td>\n",
       "      <td>married</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Peter</td>\n",
       "      <td>m</td>\n",
       "      <td>looking_for_wedding_shopping</td>\n",
       "      <td>millennials</td>\n",
       "      <td>medium</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   persona_name gender                  shopper_type      age_range  income  \\\n",
       "id                                                                            \n",
       "1           Bob      m           looking_for_bargain    millennials     low   \n",
       "2          Sara      f           looking_for_quality    baby-boomer    high   \n",
       "3         Linda      f       looking_for_inspiration   generation-x  medium   \n",
       "4           Tom      m     does_not_know_what_to_get    millennials  single   \n",
       "5          Lisa      f              shops_for_family   generation-x    high   \n",
       "6         Peter      m  looking_for_wedding_shopping    millennials  medium   \n",
       "\n",
       "   family_status  \n",
       "id                \n",
       "1         single  \n",
       "2        married  \n",
       "3         single  \n",
       "4            NaN  \n",
       "5        married  \n",
       "6         single  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reverence table of personas\n",
    "df_personas = pd.read_csv(\"personas.csv\")\n",
    "df_personas = df_personas.set_index(\"id\")\n",
    "display(df_personas)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the maximum length of the click path, I just defined in the click path generation script a bias how long the typical web session would be for the different personas. The multiplication of the bias with a random value between 1 and 3 gives us the max session length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# session length bias per persona\n",
    "SESSION_LENGTH_BIAS = {\n",
    "    \"looking_for_bargain\": 12,\n",
    "    \"looking_for_quality\": 18,\n",
    "    \"looking_for_inspiration\": 27,\n",
    "    \"does_not_know_what_to_get\": 23,\n",
    "    \"shops_for_family\": 21,\n",
    "    \"looking_for_wedding_shopping\": 25,\n",
    "}\n",
    "\n",
    "# the session length bias is used to calucate the maxium session length, which is the product\n",
    "# of a random integer between 1 and 3 multiplied with the session length bias\n",
    "def get_session_length(shopper_type):\n",
    "    return random.randint(1, 3) * SESSION_LENGTH_BIAS[shopper_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pick in the generation script the next page, after the first page was picked by the get_frist_page function, I created the get_next_page function that selects the next page, based on the selected shopper persona and the for this person defined probabilities for the next page from the page transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function to translate page ids to page names\n",
    "def id_to_page(id):\n",
    "        switcher = {\n",
    "            1: \"SEARCH_PAGE\",\n",
    "            2: \"LANDING_PAGE\",\n",
    "            3: \"DETAIL_PAGE\",\n",
    "            4: \"CATEGORY_PAGE\",\n",
    "            5: \"BLOG_POST\",\n",
    "            6: \"WISH_LIST\",\n",
    "            7: \"SHOPPING_CART\",\n",
    "            8: \"PURCHASE\",\n",
    "        }\n",
    "        return switcher.get(id, \"NOT_DEFINED_PAGE\")\n",
    "    \n",
    "# picks the first page for a persona (shopper_type) based on the persona_page_bias_matrix probabilites\n",
    "def get_frist_page(shopper_type):\n",
    "    persona_pages_bias = df_persona_page_bias_matrix.loc[shopper_type]\n",
    "    \n",
    "    # np.random.multinomial function draw samples from a multinomial distribution \n",
    "    # defined by the persona_pages_bias values\n",
    "    selected_page = np.random.multinomial(1, np.array(persona_pages_bias))\n",
    "    \n",
    "    pages = list(df_persona_page_bias_matrix.columns.values)\n",
    "    frist_page = pages[np.flatnonzero(selected_page)[0]]\n",
    "    return frist_page\n",
    "\n",
    "# picks the next page for a persona based on the page_transition_matrix probabilites.\n",
    "def get_next_page(shopper_type, previous_page):\n",
    "    df_page_transition_matrix = pd.read_csv(\"./page_transition_matrix/page_transition_matrix-\" + shopper_type + \".csv\" , sep=';', decimal= \",\")\n",
    "    df_page_transition_matrix = df_page_transition_matrix.set_index(\"PAGES\")\n",
    "    pages = list(df_page_transition_matrix.columns.values)\n",
    "    pages_transformation_distribution = df_page_transition_matrix.loc[previous_page]\n",
    "    selected_page = np.random.multinomial(1, pages_transformation_distribution)\n",
    "    page = pages[np.flatnonzero(selected_page)[0]]\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the generation script picks as the next click the event of a customer purchase, the session will end before the maximum session length is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates click sequence of a persona based on first page input and max session length\n",
    "def get_click_path(shopper_type, session_length, first_page):\n",
    "    \n",
    "    # initialize the click path with the first page picked\n",
    "    click_path = [first_page]\n",
    "    # and the the frist page to the previous page for the get_net_page in the following for loop\n",
    "    previous_page = first_page\n",
    "\n",
    "    # pick the next page based as long the max session length is not yet reached\n",
    "    for click_step in range(session_length):\n",
    "        page = get_next_page(shopper_type,previous_page)\n",
    "        # if clicked page is not the PURCHAGE page add the next page to the click path\n",
    "        if page != \"PURCHASE\":\n",
    "            # set current page to the pevious page\n",
    "            previous_page = page\n",
    "            # append picked page to the click path\n",
    "            click_path.append(page)\n",
    "        # if page is PURCHASE page the click sequence is ended even if the max session length is not yet reached    \n",
    "        else:\n",
    "            # append PURCHASE page to the click path\n",
    "            click_path.append(page)\n",
    "            # exit the for loop\n",
    "            return click_path\n",
    "    return click_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all logic to create a larger set of click sequence examples that will be stored under the file clickstream.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0 personas a clickstream was created\n",
      "For 50 personas a clickstream was created\n",
      "For 100 personas a clickstream was created\n",
      "For 150 personas a clickstream was created\n",
      "For 200 personas a clickstream was created\n",
      "For 250 personas a clickstream was created\n",
      "For 300 personas a clickstream was created\n",
      "For 350 personas a clickstream was created\n",
      "For 400 personas a clickstream was created\n",
      "For 450 personas a clickstream was created\n",
      "For 500 personas a clickstream was created\n",
      "For 550 personas a clickstream was created\n",
      "For 600 personas a clickstream was created\n",
      "For 650 personas a clickstream was created\n",
      "For 700 personas a clickstream was created\n",
      "For 750 personas a clickstream was created\n",
      "For 800 personas a clickstream was created\n",
      "For 850 personas a clickstream was created\n",
      "For 900 personas a clickstream was created\n",
      "For 950 personas a clickstream was created\n"
     ]
    }
   ],
   "source": [
    "# open clickstream csv file to capture create clickstreams\n",
    "clickstream = open('clickstream.csv', 'w')\n",
    "\n",
    "# run simulation to create clickstreams for 1000 ramdom picked personas\n",
    "for session_id in range(0,1000):\n",
    "    # get randomly one persona selected\n",
    "    shopper_type_id = random.randint(1,6)\n",
    "    # lookup the persona name\n",
    "    shopper_type = df_personas.loc[shopper_type_id,\"shopper_type\"] \n",
    "    # predict the session length\n",
    "    session_length = get_session_length(shopper_type)\n",
    "    # get first page for the selected persona\n",
    "    first_page = get_frist_page(shopper_type)\n",
    "    # create click path for persona based on page transation matrix and max session length; \n",
    "    # first page input starts the click path\n",
    "    clickpath = get_click_path(shopper_type, session_length, first_page)\n",
    "    \n",
    "    # create a List with the persona name at the start and followed by the clickpath\n",
    "    List = [shopper_type]\n",
    "    List.extend(clickpath)\n",
    "    \n",
    "    # provides status indication on the clickstream creation process, as it take a few minutes\n",
    "    if session_id % 50 == 0:\n",
    "        print (\"For \"+ str(session_id) + \" personas a clickstream was created\")\n",
    "   \n",
    "    # turn List in a string line for clickstream.csv with comma as separator between click events\n",
    "    strLine = \",\".join(List)\n",
    "    # write strLine to clickstream.csv file\n",
    "    clickstream.write(strLine + \"\\n\")\n",
    "    \n",
    "# close clickstream.csv file    \n",
    "clickstream.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 LSTM architecture for click behavior prediction\n",
    "\n",
    "In the introduction, I highlighted that the more challenging type of sequence prediction problem is to take a sequence as input and to predict a sequence as output. That kind of problem is called sequence-to-sequence prediction, or seq2seq in short. The one modeling concern that makes these problems challenging is that the length of the input and output sequences can vary.\n",
    "\n",
    "One approach to seq2seq prediction problems that have proven very eﬀective is called the Encoder-Decoder LSTM. That type of architecture is comprised of two models: one for reading the input sequence and encoding it into a ﬁxed-length vector, and a second for decoding the ﬁxed-length vector and outputting the predicted sequence.  The architecture is named  Encoder-Decoder LSTM because it uses the encoder and decoder models in concert. And because of its flexibility, it is a perfect match for our goal to predict the click behavior of a customer in Huma.\n",
    "\n",
    "Initially, the Encoder-Decoder LSTM was developed for natural language processing problems where it demonstrated state-of-the-art performance, speciﬁcally in the area of text translation called statistical machine translation. The innovation of this architecture is the use of a ﬁxed-sized internal representation at the heart of the model, and that input sequences are read to, and output sequences are read from the model. Because of the similarity to work embedding, the method is also referred to as sequence embedding.\n",
    "\n",
    "So this architecture can be used for many interesting applications:\n",
    "- Machine translation, e.g., translation from one language into another\n",
    "- Learning to execute, e.g., to calculate the outcome fo small programs\n",
    "- Image captioning, e.g., generating a text description for images\n",
    "- Conversation modeling, e.g., creating answers to textual questions\n",
    "- Movement classification, e.g., generating a sequence of commands from a sequence of gestures\n",
    "- Click behavior prediction, e.g., as envisioned for Huma\n",
    "\n",
    "As the examples show this Encoder-Decoder LSTM architecture, has great application potential of Huma not only in the use case we are looking into right now but also the conversation modeling and movement classification part, does add to the envisioned purpose of Huma to help customers proactively and understand their full context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Implement Encode-Decoder LSTM model\n",
    "\n",
    "Now let's dive into the code that implements this architecture. \n",
    "\n",
    "For my implementation, I picked the Keras library, that is a high-level API for neural networks, developed in Python and able of running on top of  Theano, TensorFlow or CNTK. It was developed with a focus on enabling fast experimentation.\n",
    "\n",
    "By using Keras, we have a deep learning library that:\n",
    "\n",
    "- allows to us easy and fast prototype (as it is very user friendliness, modular, and extensible),\n",
    "- provides support for both convolutional networks and recurrent networks, as well as combinations of the two,\n",
    "- and seamlessly runs on CPU and GPU.\n",
    "\n",
    "If you have any questions during the tutorial, you can always go to the documentation at [Keras.io](https://keras.io/).\n",
    "\n",
    "Before we can develop our code, we need to import the keras library and a utility function from sklearn lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import of sklearn lib for encoding of some of out data\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# import of the kears modules we will use\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "# when you execute the imports in your jupyter notebook you should see after the input \"Using TensorFlow backend\"\n",
    "# this indicate that you are using for this tutorial the right backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our selected Encoder-Decoder LSTM can be implemented directly in Keras. We should think of the model as being comprised of an encoder and a decoder. At first, the input sequence is shown to the network one encoded click event at a time. We need to learn the relationship between the steps of the input sequence and find an internal representation of the relationships.\n",
    "\n",
    "We can use one or more LSTM layers to implement the model for the encoder. The model output is a fixed-size vector that describes the internal representation of the input sequence. The count of memory cells in this layer defines the length of this fixed-sized vector.\n",
    "\n",
    "````python\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(steps_before, feature_count)))\n",
    "````\n",
    "\n",
    "With the decoder, we must transform the learned internal representation of the input sequence into the correct output sequence. We can use one or more LSTM layers to implement the decoder model. The decoder model reads from the fixed sized output from the encoder model. Like for  Vanilla LSTM architectures, a Dense layer is used as the output for the network. Through wrapping the Dense layer in a **TimeDistributed** wrapper, we can use the same weights for each output time step in the output sequence \n",
    "\n",
    "````python\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(feature_count,activation='softmax')))\n",
    "````\n",
    "\n",
    "But there’s a problem.  The encoder to the decoder do not fit, but we need to connect them. The reason for that is, that the encoder will produce a 2-dimensional output matrix, and where the length is defined by the count of memory cells in that layer. On the other hand, the decoder is an LSTM layer that expects a 3D input of [samples, time steps, features] to produce a decoded sequence of some different length as we define it.\n",
    "\n",
    "If we try to bring these pieces together, we will get an error indicating to us that the output of the decoder is 2D and a 3D input to the decoder is expected. That can be solved by using a RepeatVector layer. This layer simply repeats the provided 2D input multiple times to create a 3D output.\n",
    "\n",
    "The RepeatVector layer can be used as an adapter to fit the encoder and decoder parts of the network together, what is achieved by configuring the RepeatVector to repeat the fixed length vector one time for each time step in the output sequence.\n",
    "\n",
    "````python\n",
    "model.add(RepeatVector(steps_after))\n",
    "````\n",
    "\n",
    "In summary, we use the RepeatVector to fit the fixed-sized 2D output of the encoder to the differing length and 3D input expected from the decoder. By using the TimeDistributed wrapper, the same output layer can be reused for each element in the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Configure the LSTM model\n",
    "\n",
    "I use a single LSTM layer for the encoder and another single layer for the decoder. The encoder is deﬁned with 128 memory cells and the decoder with 64 memory cells. The best number of memory cells can be found with a little trial and error. The asymmetry in layer sizes in the encoder and decoder is a good fit as we will use longer input sequences defined by the **steps_before** variable as output sequences we predict, defined by the **steps_after** variable.\n",
    "\n",
    "The output layer uses the categorical log loss for the eight possible classes of click events that are predicted. We use the efficient Adam implementation of gradient descent, and we calculate the accuracy during training and model evaluation.\n",
    "\n",
    "````python\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc']) \n",
    "````\n",
    "Running the example prints a summary of the network structure. We can see that the Encoder will output a ﬁxed size vector with the length of 128 for a given input sequence. This sequence is repeated 2-times to provide a sequence of 2-time steps of 128 features to the decoder. The decoder outputs 2-time steps of 64 features to the Dense output layer that processes these one at a time via the **TimeDistributed** wrapper to output one encoded click event at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               70144     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 2, 64)             49408     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 2, 8)              520       \n",
      "=================================================================\n",
      "Total params: 120,072\n",
      "Trainable params: 120,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 410.00 337.00\" width=\"410pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-333 406,-333 406,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140220550817944 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140220550817944</title>\n",
       "<polygon fill=\"none\" points=\"101.5,-292.5 101.5,-328.5 300.5,-328.5 300.5,-292.5 101.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-306.8\">lstm_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140220550817440 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140220550817440</title>\n",
       "<polygon fill=\"none\" points=\"143,-219.5 143,-255.5 259,-255.5 259,-219.5 143,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-233.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140220550817944&#45;&gt;140220550817440 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140220550817944-&gt;140220550817440</title>\n",
       "<path d=\"M201,-292.313C201,-284.289 201,-274.547 201,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"204.5,-265.529 201,-255.529 197.5,-265.529 204.5,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140220550817328 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140220550817328</title>\n",
       "<polygon fill=\"none\" points=\"81.5,-146.5 81.5,-182.5 320.5,-182.5 320.5,-146.5 81.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-160.8\">repeat_vector_1: RepeatVector</text>\n",
       "</g>\n",
       "<!-- 140220550817440&#45;&gt;140220550817328 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140220550817440-&gt;140220550817328</title>\n",
       "<path d=\"M201,-219.313C201,-211.289 201,-201.547 201,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"204.5,-192.529 201,-182.529 197.5,-192.529 204.5,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140220550818000 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140220550818000</title>\n",
       "<polygon fill=\"none\" points=\"143,-73.5 143,-109.5 259,-109.5 259,-73.5 143,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 140220550817328&#45;&gt;140220550818000 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140220550817328-&gt;140220550818000</title>\n",
       "<path d=\"M201,-146.313C201,-138.289 201,-128.547 201,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"204.5,-119.529 201,-109.529 197.5,-119.529 204.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140220333459832 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140220333459832</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-36.5 402,-36.5 402,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201\" y=\"-14.8\">time_distributed_1(dense_1): TimeDistributed(Dense)</text>\n",
       "</g>\n",
       "<!-- 140220550818000&#45;&gt;140220333459832 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140220550818000-&gt;140220333459832</title>\n",
       "<path d=\"M201,-73.3129C201,-65.2895 201,-55.5475 201,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"204.5,-46.5288 201,-36.5288 197.5,-46.5289 204.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "steps_before = 5\n",
    "feature_count = 8\n",
    "steps_after = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(steps_before, feature_count)))\n",
    "model.add(RepeatVector(steps_after))\n",
    "model.add(LSTM(64, return_sequences=True,dropout=0.4))\n",
    "model.add(TimeDistributed(Dense(feature_count,activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc']) \n",
    "print(model.summary())\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put this model in the function create_model, so that we have the model encapsulated with all required input parameters as attributes of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(steps_before, steps_after, feature_count):\n",
    "    \"\"\" \n",
    "        creates, compiles and returns a LSTM model for sequence to sequence prediction\n",
    "        @param steps_before: the number of previous time steps (input)\n",
    "        @param steps_after: the number of posterior time steps (output or predictions)\n",
    "        @param feature_count: the number of features in the model\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(steps_before, feature_count)))\n",
    "    model.add(RepeatVector(steps_after))\n",
    "    model.add(LSTM(128, return_sequences=True,dropout=0.4))\n",
    "    model.add(TimeDistributed(Dense(feature_count,activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc']) \n",
    "    print(model.summary())\n",
    "    #plot_model(model, to_file='model.png') \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encode clickstream data for model training and validation\n",
    "\n",
    "Before we can use the clickstream data for the LSTM model training, we need to binary encode our clickstream, which consists of a string token for each page clicked, as neural networks can just work with numbers. The tokens are [\"BLOG_POST\",\"CATEGORY_PAGE\",\"DETAIL_PAGE\",\"LANDING_PAGE\",\"PURCHASE\",\"SEARCH_PAGE\",\"SHOPPING_CART\",\"WISH_LIST\"] and each token in the clickstream needs just to be represented by 1 and 0 in a binary vector. For example, [\"BLOG_POST\"] of the 8 page tokens would be represented as the following binary vector [[1 0 0 0 0 0 0 0] and [\"CATEGORY_PAGE\"] with the binary vector [0 1 0 0 0 0 0 0] and so on.\n",
    "                                                                           \n",
    "Following you find the code that does this binary encoding with the help of the **LabelBinarizer** from the sklearn lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickstream: ['BLOG_POST', 'SEARCH_PAGE', 'LANDING_PAGE', 'nan', 'WISH_LIST', 'CATEGORY_PAGE']\n",
      "Clickstream onehot encoded: [[1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# example clickstream as a list\n",
    "clickstream = [\"BLOG_POST\",\"SEARCH_PAGE\",\"LANDING_PAGE\",\"nan\",\"WISH_LIST\", \"CATEGORY_PAGE\"]\n",
    "# initializing of the LabelBinarizer from sklearn lib\n",
    "binary_encoder = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "# define the labels/tokens that should be encoded in 1 and 0's\n",
    "binary_encoder.fit([\"BLOG_POST\",\"CATEGORY_PAGE\",\"DETAIL_PAGE\",\"LANDING_PAGE\",\"PURCHASE\",\"SEARCH_PAGE\",\"SHOPPING_CART\",\"WISH_LIST\"])\n",
    "# applying the encoding\n",
    "clickstream_onehot_encoded = binary_encoder.transform(clickstream)\n",
    "# printing the clickstream as a list\n",
    "print(\"Clickstream: \"+ str(clickstream))\n",
    "# and binary encoded\n",
    "print(\"Clickstream onehot encoded: \" + str(clickstream_onehot_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this functionality a bit easier, I encapsulated it in the function **collect_onehot_encoded_clickstream**, which accepts a matrix of clickstreams and a row-index to pick a clickstream of a particular row of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clickstream_tokens = [\"SEARCH_PAGE\",\"LANDING_PAGE\",\"DETAIL_PAGE\",\"CATEGORY_PAGE\",\"BLOG_POST\",\"WISH_LIST\",\"PURCHASE\",\"SHOPPING_CART\"]\n",
    "\n",
    "def collect_onehot_encoded_clickstream(clickstreams, clickstream_id, clickstream_tokens):\n",
    "    \"\"\" \n",
    "        onehot encodes the clickstream that we feed line by line into this function\n",
    "        @param clickstreams: matrix with clickstreams in each row \n",
    "        @param clickstream_id: the row in the matix with a clickstream of a persona that should be onehot encoded\n",
    "        @param clickstream_tokens: the clickstream_tokens that should be used for the binary_encoding\n",
    "    \"\"\"\n",
    "    # retrieves the selected row from the matrix of clickstreams\n",
    "    clickstream = [str(click) for click in clickstreams[clickstream_id]]\n",
    "    # removes all entries in the click stream there the value is 'nan', which is the case, \n",
    "    # as all clickstreams have not the same length\n",
    "    clickstream = [str(click) for click in clickstream if click !='nan']\n",
    "    \n",
    "    # initialize LabelBinarizer from sklearn, which turns e.g. following click stream: [\"SEARCH_PAGE\",\"LANDING_PAGE\"]\n",
    "    # into the following onehot encoded format [[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0]]\n",
    "    binary_encoder = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder.fit(clickstream_tokens)\n",
    "    clickstream_onehot_encoded = binary_encoder.transform(clickstream)\n",
    "    \n",
    "    # onehot encoded clickstream is returned\n",
    "    return clickstream_onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have now encoded the clickstream, we need to invert the encoding to convert the output vectors back into the clickstream tokens. The following invert_encoded_clickstream() function performs this operation. To achieve this, we inverse the encoding with the **inverse_transform** function of the **LabelBinarizer** of the sklearn lib, which we initially used to encode the clickstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def invert_encoded_clickstream(sequence, clickstream_tokens):\n",
    "    \"\"\" \n",
    "        onehot encodes the clickstream that we feed line by line into this function\n",
    "        @param sequence: array with binary encoded clickstream \n",
    "        @param clickstream_tokens: the clickstream_tokens that should be used for the binary_decoding\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize LabelBinarizer from sklearn, which can decode \n",
    "    # e.g. following binary encoded click stream: [[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0]]\n",
    "    # into the following source stream [\"SEARCH_PAGE\",\"LANDING_PAGE\"] \n",
    "    binary_encoder = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder.fit(clickstream_tokens)\n",
    "    return binary_encoder.inverse_transform(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To test the invert_encoded_clickstream and to better understand what it does I have added some examples of binary encoded clickstream from the source data and the output of the prediction model. In the output vector of the prediction model we have not just zero, and one's as each feature in the vector has a probability. The inverse transform picks, in this case, the highest probability and translates that in the appropriate clickstream token.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_sequence_1_values: [[0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]]\n",
      "predicted_sequence_2_values: [[0.06 0.24 0.22 0.07 0.05 0.19 0.10 0.07]\n",
      " [0.03 0.33 0.86 0.93 0.01 0.23 0.09 0.04]]\n",
      "decoded_sequence_1: ['CATEGORY_PAGE' 'LANDING_PAGE']\n",
      "decoded_sequence_2: ['CATEGORY_PAGE' 'LANDING_PAGE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import array from numpy\n",
    "from numpy import array\n",
    "\n",
    "# encoded sequence we feed into the neural network if we decide to push two steps into it\n",
    "encoded_sequence_1 = [[0, 1, 0, 0, 0, 0, 0, 0],[0, 0, 0, 1, 0, 0, 0, 0]]\n",
    "\n",
    "# sequence we get out of the neural network as an prediction in we predict the next two steps\n",
    "predicted_sequence_2 = [[0.06, 0.24, 0.22, 0.07, 0.05, 0.19, 0.10, 0.07],[0.03, 0.33, 0.86, 0.93, 0.01, 0.23, 0.09, 0.04]]\n",
    "\n",
    "# we convert the python list into an numpy array as all inputs in the neural network are numpy arrays\n",
    "encoded_sequence_1_values = array(encoded_sequence_1)\n",
    "print(\"encoded_sequence_1_values: \" + str(encoded_sequence_1_values))\n",
    "predicted_sequence_2_values = array(predicted_sequence_2)\n",
    "print(\"predicted_sequence_2_values: \" + str(predicted_sequence_2_values))\n",
    "\n",
    "# now we feed the sequence as a numpy array into the invert_encoded_clickstream function\n",
    "decoded_sequence_1 = invert_encoded_clickstream(encoded_sequence_1_values, clickstream_tokens)\n",
    "decoded_sequence_2 = invert_encoded_clickstream(predicted_sequence_2_values, clickstream_tokens)\n",
    "\n",
    "# the printed results show the clickstream tokens again\n",
    "print(\"decoded_sequence_1: \" +str(decoded_sequence_1))\n",
    "print(\"decoded_sequence_2: \" +str(decoded_sequence_2))\n",
    "\n",
    "# as the clickstream tokens are the same we get true back from the numpy array_equal function. \n",
    "# can be used to test if prediction was correct\n",
    "np.array_equal(decoded_sequence_1,decoded_sequence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Implement training code for model\n",
    "\n",
    "Now we can create the code to fit and train the model. For that we just need to call the fit funciton of the defined keras model and pass in the function call the input sequence **dataX** and output sequence **dataY** we want to predict. The batch size I put to 32, which is another hyper parameter we can optimize by a bit trial and error, because for this parameter there is no optimal conﬁguration. The epochs variable allows us to define how often we iterate over the training data arrays. For the validation split I configured with the **validation_split** variable that 5% of the data is used to evaluate the loss at the end of each epoch. That validation data will not be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_clickstream(model, dataX, dataY, epoch_count):\n",
    "    \"\"\" \n",
    "        trains the clickstream model\n",
    "        @param model: defined model to use for training\n",
    "        @param dataX: input sequence as a numpy array in the right shape\n",
    "        @param dataY: output sequence as a numpy array in the right shape\n",
    "        @param epoch_count: defines how often we iterate over the training data arrays\n",
    "        \n",
    "    \"\"\"\n",
    "    # fit the model\n",
    "    history = model.fit(dataX, dataY, batch_size=32, epochs=epoch_count, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the training next, we will see the following progress bar that shows the loss and accuracy of the model at the end of each batch.\n",
    "````text\n",
    "1414/1414 [========================] - 2s - loss: 1.7448 - acc: 0.3971 - val_loss: 1.5074 - val_acc: 0.3478\n",
    "````\n",
    "Next we put the code together that loads the training data and feeds it in the training_clickstream function. Once the training is finished we save the model for later evaluation on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode clickstream...\n",
      "0 clickstreams encoded\n",
      "50 clickstreams encoded\n",
      "100 clickstreams encoded\n",
      "150 clickstreams encoded\n",
      "200 clickstreams encoded\n",
      "250 clickstreams encoded\n",
      "300 clickstreams encoded\n",
      "350 clickstreams encoded\n",
      "400 clickstreams encoded\n",
      "450 clickstreams encoded\n",
      "500 clickstreams encoded\n",
      "550 clickstreams encoded\n",
      "600 clickstreams encoded\n",
      "650 clickstreams encoded\n",
      "700 clickstreams encoded\n",
      "750 clickstreams encoded\n",
      "800 clickstreams encoded\n",
      "850 clickstreams encoded\n",
      "Shape of dataX: (16164, 3, 8)\n",
      "Shape of dataY: (16164, 2, 8)\n",
      "creating and fit model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               70144     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 2, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 2, 8)              1032      \n",
      "=================================================================\n",
      "Total params: 202,760\n",
      "Trainable params: 202,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15355 samples, validate on 809 samples\n",
      "Epoch 1/10\n",
      "15355/15355 [==============================] - 10s - loss: 1.4272 - acc: 0.4089 - val_loss: 1.3425 - val_acc: 0.4295\n",
      "Epoch 2/10\n",
      "15355/15355 [==============================] - 9s - loss: 1.3400 - acc: 0.4162 - val_loss: 1.3063 - val_acc: 0.4234\n",
      "Epoch 3/10\n",
      "15355/15355 [==============================] - 10s - loss: 1.3193 - acc: 0.4218 - val_loss: 1.3114 - val_acc: 0.4283\n",
      "Epoch 4/10\n",
      "15355/15355 [==============================] - 10s - loss: 1.3077 - acc: 0.4235 - val_loss: 1.2917 - val_acc: 0.4345\n",
      "Epoch 5/10\n",
      "15355/15355 [==============================] - 10s - loss: 1.2968 - acc: 0.4291 - val_loss: 1.2843 - val_acc: 0.4363\n",
      "Epoch 6/10\n",
      "15355/15355 [==============================] - 10s - loss: 1.2906 - acc: 0.4265 - val_loss: 1.2788 - val_acc: 0.4425\n",
      "Epoch 7/10\n",
      "15355/15355 [==============================] - 10s - loss: 1.2861 - acc: 0.4330 - val_loss: 1.2805 - val_acc: 0.4370\n",
      "Epoch 8/10\n",
      "15355/15355 [==============================] - 9s - loss: 1.2827 - acc: 0.4317 - val_loss: 1.2747 - val_acc: 0.4314\n",
      "Epoch 9/10\n",
      "15355/15355 [==============================] - 10s - loss: 1.2808 - acc: 0.4290 - val_loss: 1.2785 - val_acc: 0.4363\n",
      "Epoch 10/10\n",
      "15355/15355 [==============================] - 9s - loss: 1.2785 - acc: 0.4340 - val_loss: 1.2816 - val_acc: 0.4308\n",
      "serialize model to JSON...\n",
      "serialize model to HDF5...\n",
      "saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# training our clickstream predication model\n",
    "\n",
    "# load the clickstream samples from clickstream.csv that we created before \n",
    "persona_column = ['PERSONA']\n",
    "time_columns = list(range(1, 100))\n",
    "columns = persona_column + time_columns\n",
    "df_clickstream = pd.read_csv(\"clickstream.csv\", names=columns)\n",
    "df_clickstream = df_clickstream.set_index(\"PERSONA\")\n",
    "clickstream_samples = df_clickstream.values.tolist()\n",
    "\n",
    "\n",
    "# define the epochs for the model training\n",
    "EPOCHS = 10\n",
    "# define how many click events should be used as input for the model\n",
    "n_pre = 3\n",
    "# define how many click events should be used as output for the model\n",
    "n_post = 2\n",
    "# feature count of possible click event we want to predict is 8\n",
    "feature_count = 8\n",
    "\n",
    "# encode clickstream\n",
    "print('encode clickstream...')\n",
    "\n",
    "# setup two lists for click stream input and output\n",
    "dX, dY = [], []\n",
    "# iterate over the clickstream.csv file which 1000 lines. We use the first 900 for training; \n",
    "# rest will be used for testing\n",
    "for k in range(900):\n",
    "    # binary encode the clickstream\n",
    "    clickstream_onehot_encoded = collect_onehot_encoded_clickstream(clickstream_samples, k, clickstream_tokens)\n",
    "\n",
    "    # provides status indication on the clickstream encoding process\n",
    "    if k % 50 == 0:\n",
    "        print (str(k) + \" clickstreams encoded\")\n",
    "\n",
    "    # create now the input click event sequence and output click event sequence \n",
    "    # as by definition of the n_pre and n_post length\n",
    "    for i in range(len(clickstream_onehot_encoded)-n_pre-n_post):\n",
    "        dX.append(clickstream_onehot_encoded[i:i+n_pre])\n",
    "        dY.append(clickstream_onehot_encoded[i+n_pre:i+n_pre+n_post])\n",
    "\n",
    "# convert the list in an numpy array\n",
    "dataX = np.array(dX)\n",
    "dataY = np.array(dY)\n",
    "\n",
    "print(\"Shape of dataX: \" + str(dataX.shape))\n",
    "print(\"Shape of dataY: \" + str(dataY.shape))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "print('creating and fit model...')\n",
    "\n",
    "model = create_model(n_pre, n_post, feature_count)\n",
    "train_clickstream(model, dataX, dataY, EPOCHS)\n",
    "\n",
    "# serialize model to JSON\n",
    "print('serialize model to JSON...')\n",
    "model_json = model.to_json()\n",
    "with open(\"clickstream_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "print('serialize model to HDF5...')\n",
    "model.save_weights(\"clickstream_model.h5\")\n",
    "\n",
    "print(\"saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the validation data used during training, to calculate the loss and the accuracy of the predictions, we see that we will be able to predict some patterns in the click stream, but as we provide not yet more data besides the clickstream the creation of a very accurate prediction model is very difficult. That is understandable, especially if we consider that the pattern of the events got created purely based on simple probabilities for the transition from one page to another one. That does not reflect a strong longer dependance on previous occurred page events. I will show soon how we can overcome this by providing more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.5 Implement testing code for trained model\n",
    "But if we test the trained model with data it has not yet seen, we will see that the model will get quite some predictions right, for a so difficult not linear and sequence based prediction. To test the trained model we create a **test_clickstream** function that loads the saved model and its weights and does predicitons for all input sequences of data the model has not yet seen and compares it with the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "def test_clickstream(clickstream_samples, model_json, model_weights):\n",
    "    \"\"\"  \n",
    "        testing how well the network can predict\n",
    "        @param clickstream_samples: list of list with clickstream samples\n",
    "        @param model_json: filename of saved model as json\n",
    "        @param dataY: filename of saved weight of model \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # load json and create model\n",
    "    print('load saved model json...')\n",
    "    json_file = open(model_json, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    print('load saved model weights...')\n",
    "    loaded_model.load_weights(model_weights)\n",
    "    \n",
    "    print(\"loaded model from disk\")\n",
    "\n",
    "      \n",
    "    # use model to predict\n",
    "    print('use model to test and predict...')\n",
    "\n",
    "    float_formatter = lambda x: \"%.2f\" % x\n",
    "    np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "    \n",
    "    # setup two lists for click stream input and output on each iteration\n",
    "    X, Y = [], [] \n",
    "        \n",
    "    # iterate over the clickstream.csv file which 1000 lines. We used the first 900 for training; \n",
    "    # rest from 901 up to 990 will be used for testing\n",
    "    for k in range(901,990):\n",
    "         # binary encode the clickstream\n",
    "        clickstream_onehot_encoded = collect_onehot_encoded_clickstream(clickstream_samples, k,  clickstream_tokens)\n",
    "        \n",
    "               \n",
    "        # create now the input click event sequence and output click event sequence \n",
    "        # as by definition of the n_pre and n_post length and make prediction for each input\n",
    "        for i in range(len(clickstream_onehot_encoded)-n_pre-n_post):\n",
    "            \n",
    "            # get the input click event sequence and \n",
    "            X.append(clickstream_onehot_encoded[i:i+n_pre])\n",
    "            # output click event sequence \n",
    "            Y.append(clickstream_onehot_encoded[i+n_pre:i+n_pre+n_post])\n",
    "            \n",
    "            # setup two lists for click stream input and output on each iteration\n",
    "            dX, dY = [], []\n",
    "            # get the input click event sequence and \n",
    "            dX.append(clickstream_onehot_encoded[i:i+n_pre])\n",
    "            # output click event sequence \n",
    "            dY.append(clickstream_onehot_encoded[i+n_pre:i+n_pre+n_post])\n",
    "            # convert the list in an numpy array\n",
    "            dataX = np.array(dX)\n",
    "            dataY = np.array(dY)\n",
    "\n",
    "            # predict the output sequence\n",
    "            predict = model.predict(dataX)\n",
    "            \n",
    "            \n",
    "            # print out only the 50th prediction\n",
    "            if (i % 50 == 0) and (i != 0):\n",
    "                # print inputs and output\n",
    "                print(\"dataX Input:\\n\" + str(dataX))\n",
    "                print(\"Click Event Input:\\n\" + str(invert_encoded_clickstream(dataX[0], clickstream_tokens)))\n",
    "                print(\"dataY Expected:\\n\" + str(dataY))\n",
    "                print(\"dataY Predicted:\\n\"+ str(predict))\n",
    "                # print inverse decoded input and output as clickstream\n",
    "                print(\"Expected Click Event Output:\\n\" +str(invert_encoded_clickstream(dataY[0], clickstream_tokens)))\n",
    "                print(\"Predicted Click Event Output:\\n\" + str(invert_encoded_clickstream(predict[0], clickstream_tokens)))\n",
    "                # print status if prediciton was correct or not\n",
    "                print(\"Prediction: \" + str(np.array_equal(invert_encoded_clickstream(dataY[0], \n",
    "                clickstream_tokens),invert_encoded_clickstream(predict[0], clickstream_tokens))))\n",
    "            \n",
    "            \n",
    "    dX = np.array(X)\n",
    "    dY = np.array(Y)\n",
    "\n",
    "    # print loss and accuracy of model for whole test dataset\n",
    "    loss, acc = model.evaluate(dX, dY, verbose=0)\n",
    "    print('loss for full test dataset: %f, and accuracy: %f' % (loss, acc*100))\n",
    "            \n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the clickstream samples data and feed it in the **test_clickstream** function and evaluate the results based on the final Loss and Accuracy report at the end of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved model json...\n",
      "load saved model weights...\n",
      "loaded model from disk\n",
      "use model to test and predict...\n",
      "dataX Input:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 1 0 0]]]\n",
      "Click Event Input:\n",
      "['CATEGORY_PAGE' 'DETAIL_PAGE' 'SEARCH_PAGE']\n",
      "dataY Expected:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 1 0 0]]]\n",
      "dataY Predicted:\n",
      "[[[0.02 0.31 0.35 0.03 0.00 0.29 0.00 0.00]\n",
      "  [0.02 0.38 0.24 0.01 0.00 0.27 0.07 0.01]]]\n",
      "Expected Click Event Output:\n",
      "['CATEGORY_PAGE' 'SEARCH_PAGE']\n",
      "Predicted Click Event Output:\n",
      "['DETAIL_PAGE' 'CATEGORY_PAGE']\n",
      "Prediction: False\n",
      "dataX Input:\n",
      "[[[0 0 1 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]]]\n",
      "Click Event Input:\n",
      "['DETAIL_PAGE' 'CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "dataY Expected:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]]]\n",
      "dataY Predicted:\n",
      "[[[0.00 0.46 0.45 0.01 0.00 0.08 0.00 0.00]\n",
      "  [0.00 0.43 0.32 0.01 0.00 0.12 0.08 0.03]]]\n",
      "Expected Click Event Output:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "Predicted Click Event Output:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "Prediction: True\n",
      "dataX Input:\n",
      "[[[0 0 0 0 0 1 0 0]\n",
      "  [0 0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 1 0 0]]]\n",
      "Click Event Input:\n",
      "['SEARCH_PAGE' 'DETAIL_PAGE' 'SEARCH_PAGE']\n",
      "dataY Expected:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]]]\n",
      "dataY Predicted:\n",
      "[[[0.01 0.23 0.42 0.02 0.00 0.32 0.00 0.00]\n",
      "  [0.01 0.33 0.26 0.02 0.00 0.30 0.08 0.01]]]\n",
      "Expected Click Event Output:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "Predicted Click Event Output:\n",
      "['DETAIL_PAGE' 'CATEGORY_PAGE']\n",
      "Prediction: False\n",
      "dataX Input:\n",
      "[[[0 0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0 0]\n",
      "  [0 0 0 1 0 0 0 0]]]\n",
      "Click Event Input:\n",
      "['SEARCH_PAGE' 'SEARCH_PAGE' 'LANDING_PAGE']\n",
      "dataY Expected:\n",
      "[[[0 0 1 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]]]\n",
      "dataY Predicted:\n",
      "[[[0.08 0.37 0.22 0.06 0.00 0.27 0.00 0.00]\n",
      "  [0.05 0.43 0.20 0.02 0.00 0.26 0.04 0.01]]]\n",
      "Expected Click Event Output:\n",
      "['DETAIL_PAGE' 'CATEGORY_PAGE']\n",
      "Predicted Click Event Output:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "Prediction: False\n",
      "dataX Input:\n",
      "[[[0 0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0 0]\n",
      "  [0 1 0 0 0 0 0 0]]]\n",
      "Click Event Input:\n",
      "['SEARCH_PAGE' 'SEARCH_PAGE' 'CATEGORY_PAGE']\n",
      "dataY Expected:\n",
      "[[[0 0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1 0]]]\n",
      "dataY Predicted:\n",
      "[[[0.00 0.47 0.31 0.00 0.00 0.21 0.00 0.00]\n",
      "  [0.01 0.43 0.23 0.01 0.00 0.28 0.04 0.01]]]\n",
      "Expected Click Event Output:\n",
      "['DETAIL_PAGE' 'SHOPPING_CART']\n",
      "Predicted Click Event Output:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "Prediction: False\n",
      "dataX Input:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]]]\n",
      "Click Event Input:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "dataY Expected:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 1 0 0 0 0 0 0]]]\n",
      "dataY Predicted:\n",
      "[[[0.00 0.51 0.36 0.01 0.00 0.11 0.00 0.00]\n",
      "  [0.00 0.45 0.27 0.01 0.00 0.18 0.06 0.02]]]\n",
      "Expected Click Event Output:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "Predicted Click Event Output:\n",
      "['CATEGORY_PAGE' 'CATEGORY_PAGE']\n",
      "Prediction: True\n",
      "dataX Input:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 1 0 0]]]\n",
      "Click Event Input:\n",
      "['CATEGORY_PAGE' 'DETAIL_PAGE' 'SEARCH_PAGE']\n",
      "dataY Expected:\n",
      "[[[0 0 1 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0 0]]]\n",
      "dataY Predicted:\n",
      "[[[0.02 0.31 0.35 0.03 0.00 0.29 0.00 0.00]\n",
      "  [0.02 0.38 0.24 0.01 0.00 0.27 0.07 0.01]]]\n",
      "Expected Click Event Output:\n",
      "['DETAIL_PAGE' 'DETAIL_PAGE']\n",
      "Predicted Click Event Output:\n",
      "['DETAIL_PAGE' 'CATEGORY_PAGE']\n",
      "Prediction: False\n",
      "loss for full test dataset: 1.314191, and accuracy: 41.915274\n"
     ]
    }
   ],
   "source": [
    "# testing our clickstream predication model\n",
    "\n",
    "# load the clickstream samples from clickstream.csv that we created before \n",
    "persona_column = ['PERSONA']\n",
    "time_columns = list(range(1, 100))\n",
    "columns = persona_column + time_columns\n",
    "df_clickstream = pd.read_csv(\"clickstream.csv\", names=columns)\n",
    "df_clickstream = df_clickstream.set_index(\"PERSONA\")\n",
    "clickstream_samples = df_clickstream.values.tolist()\n",
    "\n",
    "test_clickstream(clickstream_samples, 'clickstream_model.json', 'clickstream_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.6 Summary of the model performance \n",
    "\n",
    "Even that the prediction is not always correct, we can see that the probability of the predicted event in second place often comes quite close to the right prediction of a click event. Now if we add more input data or make the estimation approach more sophisticated, we should get to a much better prediction results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Classify customer click behavior to understand customer intent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 Data preparation for simulated customer click behavior data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clickstream_samples():\n",
    "    persona_column = ['PERSONA']\n",
    "    time_columns = list(range(1, 100))\n",
    "    columns = persona_column + time_columns\n",
    "    df_clickstream = pd.read_csv(\"clickstream.csv\", names=columns)\n",
    "    clickstream_samples = df_clickstream.values.tolist()\n",
    "    return clickstream_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode sequence \n",
    "def one_hot_encode_label(label):\n",
    "    # one hot encode label\n",
    "    binary_encoder_label= LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_label.fit([\"does_not_know_what_to_get\",\"shops_for_family\",\"looking_for_inspiration\",\"looking_for_wedding_shopping\",\"looking_for_quality\",\"looking_for_bargain\"])\n",
    "    return binary_encoder_label.transform([label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decode a one hot encoded string \n",
    "def one_hot_decode_label(encoded_label):\n",
    "    binary_encoder_label= LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_label.fit([\"does_not_know_what_to_get\",\"shops_for_family\",\"looking_for_inspiration\",\"looking_for_wedding_shopping\",\"looking_for_quality\",\"looking_for_bargain\"])\n",
    "    return binary_encoder_label.inverse_transform(encoded_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get one sample from clickstream\n",
    "def get_one_clickstream_sample(clickstream_samples, clickstream_index, length, n_features_in, n_features_out, out_index):\n",
    "    \n",
    "    # generate sequence\n",
    "    clickstream = [str(click) for click in clickstream_samples[clickstream_index]]\n",
    "    \n",
    "    # one hot encode clickstream\n",
    "    binary_encoder_clickstream = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_clickstream.fit([\"SEARCH_PAGE\",\"LANDING_PAGE\",\"DETAIL_PAGE\",\"CATEGORY_PAGE\",\"BLOG_POST\",\"WISH_LIST\",\"PURCHASE\",\"SHOPPING_CART\"])\n",
    "    clickstream_onehot_encoded = binary_encoder_clickstream.transform(clickstream[1:length+1])\n",
    "  \n",
    "    # one hot encode label\n",
    "    label_onehot_encoded = one_hot_encode_label(clickstream[0])\n",
    "\n",
    "\n",
    "    # reshape sequence to 3D\n",
    "    X = clickstream_onehot_encoded.reshape((1, length, n_features_in))\n",
    "    \n",
    "    # reshape output to 2D\n",
    "    y = label_onehot_encoded.reshape(1, n_features_out)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 LSTM architecture for click behavior classification\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 10, 80)            28480     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10, 80)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 10, 125)           103000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 125)           0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 75)                60300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 456       \n",
      "=================================================================\n",
      "Total params: 192,236\n",
      "Trainable params: 192,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "length = 10\n",
    "n_features_in = 8\n",
    "n_features_out = 6\n",
    "out_index = 0\n",
    "model = Sequential()\n",
    "model.add(LSTM(80, return_sequences=True, input_shape=(length, n_features_in)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(125,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(75))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_features_out, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3 Implement training code for model¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 10, 8)\n",
      "(700, 6)\n",
      "Train on 665 samples, validate on 35 samples\n",
      "Epoch 1/30\n",
      "665/665 [==============================] - 2s - loss: 1.6792 - acc: 0.3669 - val_loss: 1.2580 - val_acc: 0.7143\n",
      "Epoch 2/30\n",
      "665/665 [==============================] - 1s - loss: 1.0720 - acc: 0.6451 - val_loss: 0.6062 - val_acc: 0.7429\n",
      "Epoch 3/30\n",
      "665/665 [==============================] - 1s - loss: 0.7706 - acc: 0.7263 - val_loss: 0.4615 - val_acc: 0.8857\n",
      "Epoch 4/30\n",
      "665/665 [==============================] - 1s - loss: 0.6833 - acc: 0.7699 - val_loss: 0.4223 - val_acc: 0.8571\n",
      "Epoch 5/30\n",
      "665/665 [==============================] - 1s - loss: 0.6378 - acc: 0.7669 - val_loss: 0.4062 - val_acc: 0.8857\n",
      "Epoch 6/30\n",
      "665/665 [==============================] - 1s - loss: 0.5811 - acc: 0.8060 - val_loss: 0.2842 - val_acc: 0.9429\n",
      "Epoch 7/30\n",
      "665/665 [==============================] - 1s - loss: 0.5764 - acc: 0.7880 - val_loss: 0.2743 - val_acc: 0.8571\n",
      "Epoch 8/30\n",
      "665/665 [==============================] - 1s - loss: 0.5375 - acc: 0.8030 - val_loss: 0.3311 - val_acc: 0.9143\n",
      "Epoch 9/30\n",
      "665/665 [==============================] - 1s - loss: 0.5419 - acc: 0.8195 - val_loss: 0.3351 - val_acc: 0.9143\n",
      "Epoch 10/30\n",
      "665/665 [==============================] - 1s - loss: 0.5273 - acc: 0.8150 - val_loss: 0.3230 - val_acc: 0.9429\n",
      "Epoch 11/30\n",
      "665/665 [==============================] - 1s - loss: 0.4851 - acc: 0.8256 - val_loss: 0.3070 - val_acc: 0.9143\n",
      "Epoch 12/30\n",
      "665/665 [==============================] - 1s - loss: 0.4961 - acc: 0.8316 - val_loss: 0.2632 - val_acc: 0.9143\n",
      "Epoch 13/30\n",
      "665/665 [==============================] - 1s - loss: 0.4703 - acc: 0.8316 - val_loss: 0.3036 - val_acc: 0.9143\n",
      "Epoch 14/30\n",
      "665/665 [==============================] - 1s - loss: 0.4711 - acc: 0.8286 - val_loss: 0.2401 - val_acc: 0.9429\n",
      "Epoch 15/30\n",
      "665/665 [==============================] - 1s - loss: 0.4705 - acc: 0.8361 - val_loss: 0.3285 - val_acc: 0.9143\n",
      "Epoch 16/30\n",
      "665/665 [==============================] - 1s - loss: 0.4820 - acc: 0.8361 - val_loss: 0.2580 - val_acc: 0.9429\n",
      "Epoch 17/30\n",
      "665/665 [==============================] - 1s - loss: 0.4693 - acc: 0.8195 - val_loss: 0.3210 - val_acc: 0.8857\n",
      "Epoch 18/30\n",
      "665/665 [==============================] - 1s - loss: 0.4806 - acc: 0.8406 - val_loss: 0.2496 - val_acc: 0.9143\n",
      "Epoch 19/30\n",
      "665/665 [==============================] - 1s - loss: 0.4485 - acc: 0.8436 - val_loss: 0.3120 - val_acc: 0.8571\n",
      "Epoch 20/30\n",
      "665/665 [==============================] - 1s - loss: 0.4227 - acc: 0.8466 - val_loss: 0.3235 - val_acc: 0.9143\n",
      "Epoch 21/30\n",
      "665/665 [==============================] - 1s - loss: 0.4515 - acc: 0.8271 - val_loss: 0.3095 - val_acc: 0.9143\n",
      "Epoch 22/30\n",
      "665/665 [==============================] - 1s - loss: 0.4213 - acc: 0.8541 - val_loss: 0.3600 - val_acc: 0.8571\n",
      "Epoch 23/30\n",
      "665/665 [==============================] - 1s - loss: 0.4276 - acc: 0.8421 - val_loss: 0.2918 - val_acc: 0.8857\n",
      "Epoch 24/30\n",
      "665/665 [==============================] - 1s - loss: 0.4003 - acc: 0.8481 - val_loss: 0.3536 - val_acc: 0.8571\n",
      "Epoch 25/30\n",
      "665/665 [==============================] - 1s - loss: 0.4226 - acc: 0.8391 - val_loss: 0.2906 - val_acc: 0.8857\n",
      "Epoch 26/30\n",
      "665/665 [==============================] - 1s - loss: 0.3798 - acc: 0.8466 - val_loss: 0.4060 - val_acc: 0.8571\n",
      "Epoch 27/30\n",
      "665/665 [==============================] - 1s - loss: 0.4007 - acc: 0.8571 - val_loss: 0.2778 - val_acc: 0.8857\n",
      "Epoch 28/30\n",
      "665/665 [==============================] - 1s - loss: 0.3598 - acc: 0.8647 - val_loss: 0.2963 - val_acc: 0.9143\n",
      "Epoch 29/30\n",
      "665/665 [==============================] - 1s - loss: 0.3903 - acc: 0.8526 - val_loss: 0.4045 - val_acc: 0.8571\n",
      "Epoch 30/30\n",
      "665/665 [==============================] - 1s - loss: 0.3997 - acc: 0.8511 - val_loss: 0.2779 - val_acc: 0.9143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8780bd4f98>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "clickstream_samples = get_clickstream_samples()\n",
    "dX, dY = [], []\n",
    "samples_count = 700\n",
    "for i in range(samples_count):\n",
    "    X, y = get_one_clickstream_sample(clickstream_samples, i, length, n_features_in, n_features_out, out_index)\n",
    "    dX.append(X)\n",
    "    dY.append(y)\n",
    "dataX = np.array(dX)\n",
    "dataY = np.array(dY)\n",
    "dataX = dataX.reshape(samples_count, length, n_features_in)\n",
    "dataY = dataY.reshape(samples_count, n_features_out)\n",
    "print(dataX.shape)\n",
    "print(dataY.shape)\n",
    "model.fit(dataX, dataY, batch_size=32, epochs=30, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 Implement testing code for trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.500000\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "correct = 0\n",
    "for i in range(100,500):\n",
    "    X, y = get_one_clickstream_sample(clickstream_samples, i, length, n_features_in, n_features_out, out_index)\n",
    "    yhat = model.predict(X)\n",
    "    if one_hot_decode_label(yhat) == one_hot_decode_label(y):\n",
    "        correct += 1\n",
    "print('Accuracy: %f' % ((correct/400)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.500000\n"
     ]
    }
   ],
   "source": [
    "# prediction on new data\n",
    "correct = 0\n",
    "for i in range(791,991):\n",
    "    X, y = get_one_clickstream_sample(clickstream_samples, i, length, n_features_in, n_features_out, out_index)\n",
    "    yhat = model.predict(X)\n",
    "    if one_hot_decode_label(yhat) == one_hot_decode_label(y):\n",
    "        correct += 1\n",
    "print('Accuracy: %f' % ((correct/200)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.5 Summary of the model performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Predict customer intent based on sequence and  non sequence input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data preparation for adding non sequence input data to our simulated clickstream data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moment_column = ['MOMENT']\n",
    "time_columns = list(range(1, 100))\n",
    "columns = moment_column + time_columns\n",
    "\n",
    "df_clickstream = pd.read_csv(\"clickstream.csv\", names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. feature\n",
    "def get_number_product_colors(moment_label):\n",
    "    return {\n",
    "        'looking_for_bargain' : random.randint(1,5),\n",
    "        'looking_for_quality': random.randint(1,5),\n",
    "        'looking_for_inspiration': random.randint(5,10),\n",
    "        'does_not_know_what_to_get': random.randint(8,10),\n",
    "        'shops_for_family': random.randint(1,10),\n",
    "        'looking_for_wedding_shopping': random.randint(1,3),\n",
    "    }[moment_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. feature\n",
    "def get_number_product_categories(moment_label):\n",
    "    return {\n",
    "        'looking_for_bargain' : random.randint(1,5),\n",
    "        'looking_for_quality': random.randint(3,10),\n",
    "        'looking_for_inspiration': random.randint(5,12),\n",
    "        'does_not_know_what_to_get': random.randint(5,20),\n",
    "        'shops_for_family': random.randint(5,15),\n",
    "        'looking_for_wedding_shopping': random.randint(5,18),\n",
    "    }[moment_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3.- 5. feature\n",
    "def get_everage_price_category(moment_label):\n",
    "    return {\n",
    "        'looking_for_bargain' : \"low\",\n",
    "        'looking_for_quality': \"high\",\n",
    "        'looking_for_inspiration': random.choice([\"low\",\"medium\",\"high\"]),\n",
    "        'does_not_know_what_to_get': random.choice([\"low\",\"medium\",\"high\"]),\n",
    "        'shops_for_family': random.choice([\"low\",\"medium\"]),\n",
    "        'looking_for_wedding_shopping': random.choice([\"medium\",\"high\"]),\n",
    "    }[moment_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6.- 7. feature\n",
    "def get_cloth_for_which_gender(moment_label):\n",
    "    return {\n",
    "        'looking_for_bargain' : \"one_gender\",\n",
    "        'looking_for_quality': \"one_gender\",\n",
    "        'looking_for_inspiration': \"one_gender\",\n",
    "        'does_not_know_what_to_get': \"one_gender\",\n",
    "        'shops_for_family': \"both_gender\",\n",
    "        'looking_for_wedding_shopping': \"one_gender\",\n",
    "    }[moment_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8.- 10. feature\n",
    "def get_location_of_customer(moment_label):\n",
    "    return {\n",
    "        'looking_for_bargain' : random.choice([\"home\",\"work\"]),\n",
    "        'looking_for_quality': \"home\",\n",
    "        'looking_for_inspiration': random.choice([\"travelling\",\"work\"]),\n",
    "        'does_not_know_what_to_get': random.choice([\"travelling\",\"work\"]),\n",
    "        'shops_for_family': \"home\",\n",
    "        'looking_for_wedding_shopping': \"home\",\n",
    "    }[moment_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#11.- 14. feature\n",
    "def get_sematic_time_of_session(moment_label):\n",
    "    return {\n",
    "        'looking_for_bargain' : random.choice([\"morning\",\"lunch\"]),\n",
    "        'looking_for_quality': \"evening\",\n",
    "        'looking_for_inspiration': random.choice([\"morning\",\"lunch\"]),\n",
    "        'does_not_know_what_to_get': random.choice([\"morning\",\"lunch\",\"afternoon\"]),\n",
    "        'shops_for_family': random.choice([\"evening\",\"afternoon\"]),\n",
    "        'looking_for_wedding_shopping': \"evening\",\n",
    "    }[moment_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#15.- 20. feature\n",
    "def get_emotional_state(moment_label):\n",
    "    return {\n",
    "        'looking_for_bargain' : random.choice([\"happy\",\"excited\"]),\n",
    "        'looking_for_quality': \"neutral\",\n",
    "        'looking_for_inspiration': random.choice([\"neutral\",\"excited\"]),\n",
    "        'does_not_know_what_to_get': random.choice([\"neutral\",\"sad\",\"angry\"]),\n",
    "        'shops_for_family': random.choice([\"neutral\",\"tender\"]),\n",
    "        'looking_for_wedding_shopping': random.choice([\"happy\",\"tender\"]),\n",
    "    }[moment_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0 clickstreams more features are added\n",
      "For 50 clickstreams more features are added\n",
      "For 100 clickstreams more features are added\n",
      "For 150 clickstreams more features are added\n",
      "For 200 clickstreams more features are added\n",
      "For 250 clickstreams more features are added\n",
      "For 300 clickstreams more features are added\n",
      "For 350 clickstreams more features are added\n",
      "For 400 clickstreams more features are added\n",
      "For 450 clickstreams more features are added\n",
      "For 500 clickstreams more features are added\n",
      "For 550 clickstreams more features are added\n",
      "For 600 clickstreams more features are added\n",
      "For 650 clickstreams more features are added\n",
      "For 700 clickstreams more features are added\n",
      "For 750 clickstreams more features are added\n",
      "For 800 clickstreams more features are added\n",
      "For 850 clickstreams more features are added\n",
      "For 900 clickstreams more features are added\n",
      "For 950 clickstreams more features are added\n"
     ]
    }
   ],
   "source": [
    "# open clickstream augmented csv file to capture augmented clickstreams\n",
    "clickstream_augmented = open('clickstream_augmented.csv', 'w')\n",
    "\n",
    "# the augmented clickstream data will look like this\n",
    "# SESSION_ID, MOMENT_LABEL, 10 CLICK_EVENTS, NUMBER_PRODUCT_COLORS_LOOKED_AT, NUMBER_PRODUCT_CATEGORIES_LOOKED_AT, AVERAGE_PRICE_CATEGORY_LOOKED_AT, CLOTH_FOR_WHICH_GENDER_LOOKED_AT, LOCATION_CUSTOMER, SEMANTIC_TIME, EMOTIONAL_RESPONSE\n",
    "\n",
    "strHeader = \"SESSION_ID,MOMENT_LABEL,0,1,2,3,4,5,6,7,8,9,NUMBER_PRODUCT_COLORS_LOOKED_AT,NUMBER_PRODUCT_CATEGORIES_LOOKED_AT,AVERAGE_PRICE_CATEGORY_LOOKED_AT,CLOTH_FOR_WHICH_GENDER_LOOKED_AT,LOCATION_CUSTOMER,SEMANTIC_TIME,EMOTIONAL_RESPONSE\"\n",
    "\n",
    "# write line of augmented clickstream to file\n",
    "clickstream_augmented.write(strHeader + \"\\n\")\n",
    "\n",
    "for idx, row in df_clickstream.iterrows():\n",
    "    Line = [str(idx)]\n",
    "    \n",
    "    moment_label = row[\"MOMENT\"]\n",
    "    \n",
    "    Line.append(str(moment_label))\n",
    "    clickstream = [str(click) for click in row[1:11].values]\n",
    "    Line.extend(clickstream)\n",
    "    Line.append(str(get_number_product_colors(moment_label)))\n",
    "    Line.append(str(get_number_product_categories(moment_label)))\n",
    "    Line.append(str(get_everage_price_category(moment_label)))\n",
    "    Line.append(str(get_cloth_for_which_gender(moment_label)))\n",
    "    Line.append(str(get_location_of_customer(moment_label)))\n",
    "    Line.append(str(get_sematic_time_of_session(moment_label)))\n",
    "    Line.append(str(get_emotional_state(moment_label)))\n",
    "    \n",
    "    \n",
    "    # provides status indication on the clickstream augmentation process\n",
    "    if idx % 50 == 0:\n",
    "        print (\"For \"+ str(idx) + \" clickstreams more features are added\")\n",
    "        \n",
    "    # turn list in string with comma as separate between events\n",
    "    strLine = \",\".join(Line)\n",
    "    # write line of augmented clickstream to file\n",
    "    clickstream_augmented.write(strLine + \"\\n\")\n",
    "    \n",
    "# close clickstream file    \n",
    "clickstream_augmented.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### 4.2 Helper functions to encode our data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_augmented_clickstream_samples():\n",
    "\n",
    "    clickstream_range = [i for i in range(1,12)]\n",
    "    df_clickstream = pd.read_csv(\"clickstream_augmented.csv\", usecols=clickstream_range)\n",
    "\n",
    "    clickstream_samples = df_clickstream.values.tolist()\n",
    "    return clickstream_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_session_samples():\n",
    "\n",
    "    session_range = [i for i in range(12,19)]\n",
    "    df_session = pd.read_csv(\"clickstream_augmented.csv\", usecols=session_range)\n",
    "\n",
    "    session_samples = df_session.values.tolist()\n",
    "    return session_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode sequence \n",
    "def one_hot_encode_price_category(price_category):\n",
    "    # one hot encode label\n",
    "    binary_encoder_label= LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_label.fit([\"high\",\"medium\",\"low\"])\n",
    "    return binary_encoder_label.transform([price_category])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode sequence \n",
    "def one_hot_encode_cloth_for_gender(cloth_for_gender):\n",
    "    # one hot encode label\n",
    "    binary_encoder_label= LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_label.fit([\"one_gender\",\"both_gender\"])\n",
    "    return binary_encoder_label.transform([cloth_for_gender])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode sequence \n",
    "def one_hot_encode_location_customer(location_customer):\n",
    "    # one hot encode label\n",
    "    binary_encoder_label= LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_label.fit([\"home\",\"work\",\"travelling\"])\n",
    "    return binary_encoder_label.transform([location_customer])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode sequence \n",
    "def one_hot_encode_sementic_time(sementic_time):\n",
    "    # one hot encode label\n",
    "    binary_encoder_label= LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_label.fit([\"morning\",\"lunch\",\"afternoon\",\"evening\"])\n",
    "    return binary_encoder_label.transform([sementic_time])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode sequence \n",
    "def one_hot_encode_emotional_response(emotional_response):\n",
    "    # one hot encode label\n",
    "    binary_encoder_label= LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    binary_encoder_label.fit([\"happy\",\"excited\",\"sad\",\"neutral\",\"tender\",\"angry\"])\n",
    "    return binary_encoder_label.transform([emotional_response])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate one example\n",
    "def get_one_session_sample(session_samples, session_index, session_length, session_features_in):\n",
    "    # generate sequence\n",
    "    session = [str(session_attribute) for session_attribute in session_samples[session_index]]\n",
    "\n",
    "    one_hot_encoded_session_attributes = []\n",
    "    one_hot_encoded_session_attributes.append(float(session[0])/10)\n",
    "    one_hot_encoded_session_attributes.append(float(session[1])/20)\n",
    "    one_hot_encoded_session_attributes.extend(one_hot_encode_price_category(session[2]))\n",
    "    one_hot_encoded_session_attributes.extend(one_hot_encode_cloth_for_gender(session[3]))\n",
    "    one_hot_encoded_session_attributes.extend(one_hot_encode_location_customer(session[4]))\n",
    "    one_hot_encoded_session_attributes.extend(one_hot_encode_sementic_time(session[5]))\n",
    "    one_hot_encoded_session_attributes.extend(one_hot_encode_emotional_response(session[6]))\n",
    "\n",
    "    Z = np.array(one_hot_encoded_session_attributes).reshape((1, session_length, session_features_in))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 LSTM architecture to combine sequence and not sequence data for a customer behavior classification model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 10, 8)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                   (None, 10, 80)        28480       main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                   (None, 10, 125)       103000      lstm_20[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "aux_input (InputLayer)           (None, 19)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                   (None, 75)            60300       lstm_21[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 75)            1500        aux_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)            (None, 75)            0           lstm_22[0][0]                    \n",
      "                                                                   dense_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 50)            3800        multiply_3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 6)             306         dense_13[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 197,386\n",
      "Trainable params: 197,386\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras as ke\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# define model\n",
    "length = 10\n",
    "n_features_in = 8\n",
    "n_features_out = 6\n",
    "session_length = 1\n",
    "session_features_in = 19\n",
    "out_index = 0\n",
    "\n",
    "main_input = Input(shape=(length, n_features_in), name='main_input')\n",
    "\n",
    "auxiliary_input = Input(shape=(session_features_in,), name='aux_input')\n",
    "lstm_out_1 = LSTM(80, return_sequences=True)(main_input)\n",
    "lstm_out_2 = LSTM(125, return_sequences=True)(lstm_out_1)\n",
    "lstm_out_3 = LSTM(75)(lstm_out_2)\n",
    "dense_out_1 = Dense(75)(auxiliary_input)\n",
    "multiply_out = ke.layers.multiply([lstm_out_3, dense_out_1])\n",
    "dense_out_2 = Dense(50)(multiply_out)\n",
    "main_output = Dense(n_features_out, activation='softmax')(dense_out_2)\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Implement training code for model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 10, 8)\n",
      "(700, 6)\n",
      "(700, 19)\n",
      "Train on 665 samples, validate on 35 samples\n",
      "Epoch 1/10\n",
      "665/665 [==============================] - 2s - loss: 1.5313 - acc: 0.6090 - val_loss: 0.8186 - val_acc: 0.8286\n",
      "Epoch 2/10\n",
      "665/665 [==============================] - 1s - loss: 0.5117 - acc: 0.9083 - val_loss: 0.3445 - val_acc: 0.8857\n",
      "Epoch 3/10\n",
      "665/665 [==============================] - 1s - loss: 0.2298 - acc: 0.9549 - val_loss: 0.2131 - val_acc: 0.8857\n",
      "Epoch 4/10\n",
      "665/665 [==============================] - 1s - loss: 0.1504 - acc: 0.9639 - val_loss: 0.1900 - val_acc: 0.9429\n",
      "Epoch 5/10\n",
      "665/665 [==============================] - 1s - loss: 0.1151 - acc: 0.9684 - val_loss: 0.1541 - val_acc: 0.9143\n",
      "Epoch 6/10\n",
      "665/665 [==============================] - 1s - loss: 0.0971 - acc: 0.9669 - val_loss: 0.1611 - val_acc: 0.9429\n",
      "Epoch 7/10\n",
      "665/665 [==============================] - 1s - loss: 0.0851 - acc: 0.9714 - val_loss: 0.1516 - val_acc: 0.9429\n",
      "Epoch 8/10\n",
      "665/665 [==============================] - 1s - loss: 0.0680 - acc: 0.9820 - val_loss: 0.1401 - val_acc: 0.9429\n",
      "Epoch 9/10\n",
      "665/665 [==============================] - 1s - loss: 0.0547 - acc: 0.9850 - val_loss: 0.1287 - val_acc: 0.9714\n",
      "Epoch 10/10\n",
      "665/665 [==============================] - 1s - loss: 0.0474 - acc: 0.9850 - val_loss: 0.1322 - val_acc: 0.9714\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "augmented_clickstream_samples = get_augmented_clickstream_samples()\n",
    "session_samples = get_session_samples()\n",
    "\n",
    "dX, dY, dZ = [], [], []\n",
    "samples_count = 700\n",
    "for i in range(samples_count):\n",
    "    X, y = get_one_clickstream_sample(augmented_clickstream_samples, i, length, n_features_in, n_features_out, out_index)\n",
    "    dX.append(X)\n",
    "    dY.append(y)\n",
    "    Z = get_one_session_sample(session_samples, i, session_length, session_features_in)\n",
    "    #print(Z)\n",
    "    dZ.append(Z)\n",
    "  \n",
    "    \n",
    "dataX = np.array(dX)\n",
    "dataY = np.array(dY)\n",
    "dataZ = np.array(dZ)\n",
    "\n",
    "dataX = dataX.reshape(samples_count, length, n_features_in)\n",
    "dataY = dataY.reshape(samples_count, n_features_out)\n",
    "dataZ = dataZ.reshape(samples_count, session_features_in)\n",
    "print(dataX.shape)\n",
    "print(dataY.shape)\n",
    "print(dataZ.shape)\n",
    "\n",
    "history = model.fit([dataX, dataZ], [dataY], batch_size=32, epochs=10, validation_split=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.5 Implement testing code for trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.750000\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "correct = 0\n",
    "for i in range(100,500):\n",
    "    X, y = get_one_clickstream_sample(clickstream_samples, i, length, n_features_in, n_features_out, out_index)\n",
    "    Z = get_one_session_sample(session_samples, i, session_length, session_features_in)\n",
    "    dataZ = np.array(Z)\n",
    "    dataZ = dataZ.reshape(1, session_features_in)\n",
    "    yhat = model.predict([X,dataZ])\n",
    "    if one_hot_decode_label(yhat) == one_hot_decode_label(y):\n",
    "        correct += 1\n",
    "print('Accuracy: %f' % ((correct/400)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.500000\n"
     ]
    }
   ],
   "source": [
    "# prediction on new data\n",
    "correct = 0\n",
    "for i in range(791,991):\n",
    "    X, y = get_one_clickstream_sample(clickstream_samples, i, length, n_features_in, n_features_out, out_index)\n",
    "    Z = get_one_session_sample(session_samples, i, session_length, session_features_in)\n",
    "    dataZ = np.array(Z)\n",
    "    dataZ = dataZ.reshape(1, session_features_in)\n",
    "    yhat = model.predict([X,dataZ])\n",
    "    if one_hot_decode_label(yhat) == one_hot_decode_label(y):\n",
    "        correct += 1\n",
    "print('Accuracy: %f' % ((correct/200)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.6 Summary of the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Summary and futher learning resources\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
